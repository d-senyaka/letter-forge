# âš’ï¸ Letter-Forge

![Python](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python)
![PyTorch](https://img.shields.io/badge/Framework-PyTorch-red?logo=pytorch)
![Transformer](https://img.shields.io/badge/Model-Transformer-orange?logo=openai)
![Tasks](https://img.shields.io/badge/Tasks-Encoding%20%7C%20Language%20Modeling-yellow)
![License](https://img.shields.io/badge/License-MIT-green)
![Platform](https://img.shields.io/badge/Platform-Local%20%7C%20Offline-lightgrey)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)
![Last Updated](https://img.shields.io/badge/Last%20Updated-November%202025-purple)
![Project Type](https://img.shields.io/badge/Project-From%20Scratch%20Implementation-brown)

### ğŸ§  Meaning of the Name
> *Letter-Forge* symbolizes both the ancient craft of shaping written language  
> and the modern act of building models that learn linguistic structure from scratch.

> *A forge where letters learn to think.*

**Letter-Forge** is a from-scratch implementation of Transformer architectures for character-level learning and language modeling.  
It explores how attention, memory, and positional structure can emerge from simple sequences of letters - transforming raw symbols into learned meaning.

---

## ğŸ§  Overview

Letter-Forge is built to **craft language understanding from the ground up**.  
It begins with a minimalist Transformer Encoder that learns counting and pattern recognition at the character level,  
and extends to a full Transformer Language Model capable of predicting and generating text sequences.

Each component - from self-attention to positional encoding - is implemented manually to illustrate the inner mechanics of modern deep learning models.

---

## ğŸ—ï¸ Architecture Highlights

| Component | Description |
|------------|-------------|
| **Custom Transformer Encoder** | Built from first principles using PyTorch layers (`Linear`, `Softmax`, `ReLU`) â€” no off-the-shelf Transformer modules. |
| **Self-Attention Mechanism** | Implements single-head attention using learned queries, keys, and values. Visualizes attention maps between character positions. |
| **Positional Encoding** | Supports both learned and sinusoidal positional embeddings to inject order awareness into the model. |
| **Transformer Language Model (LM)** | Extends the encoder into a causal language model predicting the next character given context. |
| **Visualization & Analysis** | Generates heatmaps showing how the model â€œlooks backâ€ over previous symbols while learning structure. |

---

## ğŸ§© Project Structure

```
letter-forge/
â”‚
â”œâ”€â”€ part-1_encoder/              # Transformer Encoder (character-level)
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ lettercounting-train.txt
â”‚   â”‚   â””â”€â”€ lettercounting-dev.txt
â”‚   â”œâ”€â”€ letter_counting.py       # Driver script
â”‚   â”œâ”€â”€ transformer.py           # Core encoder + attention implementation
â”‚   â””â”€â”€ utils.py                 # Indexer & helper utilities
â”‚
â”œâ”€â”€ part-2_lm/                   # Transformer Language Model
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ text8-100k.txt
â”‚   â”‚   â”œâ”€â”€ text8-dev.txt
â”‚   â”‚   â””â”€â”€ text8-test.txt
â”‚   â”œâ”€â”€ lm.py                    # Driver & evaluation (perplexity, sanity checks)
â”‚   â”œâ”€â”€ transformer_lm.py        # LM model + training loop
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ sandbox_utils/               # Development & testing scripts
â”‚   â”œâ”€â”€ data_pipeline_verifier.py
â”‚   â”œâ”€â”€ attention_pe_module_test.py
â”‚   â”œâ”€â”€ attention_validation_suite.py
â”‚   â””â”€â”€ repro_training_logger.py
â”‚
â”œâ”€â”€ artifacts/                   # Saved models & metadata
â”œâ”€â”€ plots/                       # Attention heatmaps & visual outputs
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/<your-username>/letter-forge.git
cd letter-forge

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate     # or .\venv\Scripts\activate on Windows

# Install dependencies
pip install torch numpy matplotlib
```

---

## ğŸš€ Usage

### ğŸ§® Part 1 â€“ Transformer Encoder
Train and test the character-level counting model:

```bash
cd part-1_encoder
python letter_counting.py
```

- Predicts how many times each letter has appeared before in a sequence.  
- Visualizes attention maps showing how the model â€œlooks backâ€ over earlier tokens.

**Sample Attention Visualization:**

<img src="../plots/attn_sample_1.png" width="400"> <img src="../plots/attn_sample_2.png" width="400">

---

### ğŸ”¡ Part 2 â€“ Transformer Language Model
Train a Transformer to predict the **next character** in a text sequence:

```bash
cd part-2_lm
python lm.py --model NEURAL
```

- Learns from the **text8** dataset (100k character subset).  
- Evaluates on perplexity and token-level likelihood.  
- Produces valid probability distributions for every step.

---

## ğŸ“Š Results Summary

| Model | Task | Metric | Result |
|--------|------|---------|--------|
| **Transformer Encoder** | Character counting (BEFORE) | Accuracy | 98.3 % |
| **Transformer Encoder** | Character counting (BEFOREAFTER) | Accuracy | 97â€“99 % (tuned) |
| **Transformer LM** | Next-char prediction (text8) | Perplexity | â‰¤ 7 (target) |
| **Attention Visualization** | Pattern detection | Highlights same-character attention clusters |

> The model successfully learns to identify prior occurrences of letters and extends this ability to generate context-aware text sequences.

---

## ğŸ”¥ Key Insights

- **Self-Attention = Contextual Memory:**  
  Each token attends to relevant predecessors, forming a learned memory of prior occurrences.  
- **Positional Encoding Enables Order Awareness:**  
  Without positional information, the model treats input as a bag of symbols; with it, order emerges.  
- **From Counting to Composition:**  
  The same underlying structure that counts characters can generate language â€” showing the continuum from perception to composition.

---

## ğŸ“ Artifacts

- **Checkpoints:** `artifacts/model_*.pt`
- **Metadata:** `artifacts/run_meta.json`
- **Plots:** `plots/*.png` â€“ attention heatmaps, loss curves, etc.

---

## ğŸ§ª Sandbox Utilities

| Script | Purpose |
|---------|----------|
| `data_pipeline_verifier.py` | Validates dataset shapes and preprocessing pipeline. |
| `attention_pe_module_test.py` | Unit-tests Positional Encoding and Attention modules. |
| `attention_validation_suite.py` | Verifies attention masks and tensor consistency. |
| `repro_training_logger.py` | Reproducible 3-epoch experiment logger (saves artifacts). |

---

## ğŸª¶ Philosophy

> *Letter-Forge* is built on a simple idea:  
> that language understanding isnâ€™t magic - itâ€™s forged through repeated interaction between memory, order, and meaning.  
> By crafting each layer manually, we can see how modern intelligence emerges, one letter at a time.

---

## ğŸ§‘â€ğŸ’» Author & Maintainer
**d-senyaka**   
AI & Data Science Developer Â· Deep Learning Enthusiast Â· Language Technology Researcher  

---

## âš–ï¸ License
This project is released under the **MIT License**.  
You are free to use, modify, and distribute it with attribution.

---

## â­ Acknowledgements
- Inspired by open research in attention mechanisms and neural sequence modeling.  
- Crafted with curiosity, patience, and an appreciation for both language and logic.

> *â€œTo forge a mind of letters is to understand the art of attention.â€*
